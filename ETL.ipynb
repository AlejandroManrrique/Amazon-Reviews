{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    NOTA: El proyecto aquí presentado es realizado por un grupo de 5 personas y dividido entre ellas, algunos métodos o formas para modificar los datos pueden ser diferentes pero el resultado en dichas transformaciones es el mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Librerias Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Carga de los datos (Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta etapa del proyecto, procederemos a importar los datos. Disponemos de un conjunto de 58 archivos, que comprenden 24 tablas junto con sus respectivos archivos de metadatos. Cada uno de estos archivos será abordado de manera individual y minuciosa como parte de nuestro proceso.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Home_and_Kitchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jlines = []\n",
    "# Abrir el archivo JSON en modo lectura\n",
    "with open('Home_and_Kitchen.json', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Guardar las líneas del archivo JSON\n",
    "for line in lines:\n",
    "    jlines.append(line)\n",
    "\n",
    "# Extraer la información de las tres columnas principales del archivo json original\n",
    "overall = []\n",
    "verified = []\n",
    "reviewTime = []\n",
    "reviewerID = []\n",
    "asin = []\n",
    "style = []\n",
    "reviewerName = []\n",
    "reviewText = []\n",
    "summary = []\n",
    "unixReviewTime = []\n",
    "\n",
    "for line in jlines:\n",
    "    # Analiza cada línea como un objeto JSON\n",
    "    data = json.loads(line.strip())  # Usar strip para eliminar caracteres de nueva línea\n",
    "    # Extraer los datos del objeto JSON\n",
    "    overall.append(data.get(\"overall\", None))\n",
    "    verified.append(data.get(\"verified\", None))\n",
    "    reviewTime.append(data.get(\"reviewTime\", None))\n",
    "    reviewerID.append(data.get(\"reviewerID\", None))\n",
    "    asin.append(data.get(\"asin\", None))\n",
    "    style.append(data.get(\"style\", None))\n",
    "    reviewerName.append(data.get(\"reviewerName\", None))\n",
    "    reviewText.append(data.get(\"reviewText\", None))\n",
    "    summary.append(data.get(\"summary\", None))\n",
    "    unixReviewTime.append(data.get(\"unixReviewTime\", None))\n",
    "\n",
    "# Crear el DataFrame\n",
    "Home_and_Kitchen = pd.DataFrame()\n",
    "Home_and_Kitchen['overall'] = overall\n",
    "Home_and_Kitchen['verified'] = verified\n",
    "Home_and_Kitchen['reviewTime'] = reviewTime\n",
    "Home_and_Kitchen['reviewerID'] = reviewerID\n",
    "Home_and_Kitchen['asin'] = asin\n",
    "Home_and_Kitchen['style'] = style\n",
    "Home_and_Kitchen['reviewerName'] = reviewerName\n",
    "Home_and_Kitchen['reviewText'] = reviewText\n",
    "Home_and_Kitchen['summary'] = summary\n",
    "Home_and_Kitchen['unixReviewTime'] = unixReviewTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Kindle_Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jlines = []\n",
    "# Abrir el archivo JSON en modo lectura\n",
    "with open('Kindle_Store.json', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Guardar las líneas del archivo JSON\n",
    "for line in lines:\n",
    "    jlines.append(line)\n",
    "\n",
    "# Extraer la información de las tres columnas principales del archivo json original\n",
    "overall = []\n",
    "verified = []\n",
    "reviewTime = []\n",
    "reviewerID = []\n",
    "asin = []\n",
    "style = []\n",
    "reviewerName = []\n",
    "reviewText = []\n",
    "summary = []\n",
    "unixReviewTime = []\n",
    "\n",
    "for line in jlines:\n",
    "    # Analiza cada línea como un objeto JSON\n",
    "    data = json.loads(line.strip())  # Usar strip para eliminar caracteres de nueva línea\n",
    "    # Extraer los datos del objeto JSON\n",
    "    overall.append(data.get(\"overall\", None))\n",
    "    verified.append(data.get(\"verified\", None))\n",
    "    reviewTime.append(data.get(\"reviewTime\", None))\n",
    "    reviewerID.append(data.get(\"reviewerID\", None))\n",
    "    asin.append(data.get(\"asin\", None))\n",
    "    style.append(data.get(\"style\", None))\n",
    "    reviewerName.append(data.get(\"reviewerName\", None))\n",
    "    reviewText.append(data.get(\"reviewText\", None))\n",
    "    summary.append(data.get(\"summary\", None))\n",
    "    unixReviewTime.append(data.get(\"unixReviewTime\", None))\n",
    "\n",
    "# Crear el DataFrame\n",
    "Kindle_Store = pd.DataFrame()\n",
    "Kindle_Store['overall'] = overall\n",
    "Kindle_Store['verified'] = verified\n",
    "Kindle_Store['reviewTime'] = reviewTime\n",
    "Kindle_Store['reviewerID'] = reviewerID\n",
    "Kindle_Store['asin'] = asin\n",
    "Kindle_Store['style'] = style\n",
    "Kindle_Store['reviewerName'] = reviewerName\n",
    "Kindle_Store['reviewText'] = reviewText\n",
    "Kindle_Store['summary'] = summary\n",
    "Kindle_Store['unixReviewTime'] = unixReviewTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Industrial_and_Scientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jlines = []\n",
    "# Abrir el archivo JSON en modo lectura\n",
    "with open('Industrial_and_Scientific.json', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Guardar las líneas del archivo JSON\n",
    "for line in lines:\n",
    "    jlines.append(line)\n",
    "\n",
    "# Extraer la información de las tres columnas principales del archivo json original\n",
    "overall = []\n",
    "verified = []\n",
    "reviewTime = []\n",
    "reviewerID = []\n",
    "asin = []\n",
    "style = []\n",
    "reviewerName = []\n",
    "reviewText = []\n",
    "summary = []\n",
    "unixReviewTime = []\n",
    "\n",
    "for line in jlines:\n",
    "    # Analiza cada línea como un objeto JSON\n",
    "    data = json.loads(line.strip())  # Usar strip para eliminar caracteres de nueva línea\n",
    "    # Extraer los datos del objeto JSON\n",
    "    overall.append(data.get(\"overall\", None))\n",
    "    verified.append(data.get(\"verified\", None))\n",
    "    reviewTime.append(data.get(\"reviewTime\", None))\n",
    "    reviewerID.append(data.get(\"reviewerID\", None))\n",
    "    asin.append(data.get(\"asin\", None))\n",
    "    style.append(data.get(\"style\", None))\n",
    "    reviewerName.append(data.get(\"reviewerName\", None))\n",
    "    reviewText.append(data.get(\"reviewText\", None))\n",
    "    summary.append(data.get(\"summary\", None))\n",
    "    unixReviewTime.append(data.get(\"unixReviewTime\", None))\n",
    "\n",
    "# Crear el DataFrame\n",
    "Industrial_and_Scientific = pd.DataFrame()\n",
    "Industrial_and_Scientific['overall'] = overall\n",
    "Industrial_and_Scientific['verified'] = verified\n",
    "Industrial_and_Scientific['reviewTime'] = reviewTime\n",
    "Industrial_and_Scientific['reviewerID'] = reviewerID\n",
    "Industrial_and_Scientific['asin'] = asin\n",
    "Industrial_and_Scientific['style'] = style\n",
    "Industrial_and_Scientific['reviewerName'] = reviewerName\n",
    "Industrial_and_Scientific['reviewText'] = reviewText\n",
    "Industrial_and_Scientific['summary'] = summary\n",
    "Industrial_and_Scientific['unixReviewTime'] = unixReviewTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Appliances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jlines = []\n",
    "# Abrir el archivo JSON en modo lectura\n",
    "with open('Appliances.json', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Guardar las líneas del archivo JSON\n",
    "for line in lines:\n",
    "    jlines.append(line)\n",
    "\n",
    "# Extraer la información de las tres columnas principales del archivo json original\n",
    "overall = []\n",
    "verified = []\n",
    "reviewTime = []\n",
    "reviewerID = []\n",
    "asin = []\n",
    "style = []\n",
    "reviewerName = []\n",
    "reviewText = []\n",
    "summary = []\n",
    "unixReviewTime = []\n",
    "\n",
    "for line in jlines:\n",
    "    # Analiza cada línea como un objeto JSON\n",
    "    data = json.loads(line.strip())  # Usar strip para eliminar caracteres de nueva línea\n",
    "    # Extraer los datos del objeto JSON\n",
    "    overall.append(data.get(\"overall\", None))\n",
    "    verified.append(data.get(\"verified\", None))\n",
    "    reviewTime.append(data.get(\"reviewTime\", None))\n",
    "    reviewerID.append(data.get(\"reviewerID\", None))\n",
    "    asin.append(data.get(\"asin\", None))\n",
    "    style.append(data.get(\"style\", None))\n",
    "    reviewerName.append(data.get(\"reviewerName\", None))\n",
    "    reviewText.append(data.get(\"reviewText\", None))\n",
    "    summary.append(data.get(\"summary\", None))\n",
    "    unixReviewTime.append(data.get(\"unixReviewTime\", None))\n",
    "\n",
    "# Crear el DataFrame\n",
    "Appliances = pd.DataFrame()\n",
    "Appliances['overall'] = overall\n",
    "Appliances['verified'] = verified\n",
    "Appliances['reviewTime'] = reviewTime\n",
    "Appliances['reviewerID'] = reviewerID\n",
    "Appliances['asin'] = asin\n",
    "Appliances['style'] = style\n",
    "Appliances['reviewerName'] = reviewerName\n",
    "Appliances['reviewText'] = reviewText\n",
    "Appliances['summary'] = summary\n",
    "Appliances['unixReviewTime'] = unixReviewTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Office_Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jlines = []\n",
    "# Abrir el archivo JSON en modo lectura\n",
    "with open('Office_Products.json', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Guardar las líneas del archivo JSON\n",
    "for line in lines:\n",
    "    jlines.append(line)\n",
    "\n",
    "# Extraer la información de las tres columnas principales del archivo json original\n",
    "overall = []\n",
    "verified = []\n",
    "reviewTime = []\n",
    "reviewerID = []\n",
    "asin = []\n",
    "style = []\n",
    "reviewerName = []\n",
    "reviewText = []\n",
    "summary = []\n",
    "unixReviewTime = []\n",
    "\n",
    "for line in jlines:\n",
    "    # Analiza cada línea como un objeto JSON\n",
    "    data = json.loads(line.strip())  # Usar strip para eliminar caracteres de nueva línea\n",
    "    # Extraer los datos del objeto JSON\n",
    "    overall.append(data.get(\"overall\", None))\n",
    "    verified.append(data.get(\"verified\", None))\n",
    "    reviewTime.append(data.get(\"reviewTime\", None))\n",
    "    reviewerID.append(data.get(\"reviewerID\", None))\n",
    "    asin.append(data.get(\"asin\", None))\n",
    "    style.append(data.get(\"style\", None))\n",
    "    reviewerName.append(data.get(\"reviewerName\", None))\n",
    "    reviewText.append(data.get(\"reviewText\", None))\n",
    "    summary.append(data.get(\"summary\", None))\n",
    "    unixReviewTime.append(data.get(\"unixReviewTime\", None))\n",
    "\n",
    "# Crear el DataFrame\n",
    "Office_Products = pd.DataFrame()\n",
    "Office_Products['overall'] = overall\n",
    "Office_Products['verified'] = verified\n",
    "Office_Products['reviewTime'] = reviewTime\n",
    "Office_Products['reviewerID'] = reviewerID\n",
    "Office_Products['asin'] = asin\n",
    "Office_Products['style'] = style\n",
    "Office_Products['reviewerName'] = reviewerName\n",
    "Office_Products['reviewText'] = reviewText\n",
    "Office_Products['summary'] = summary\n",
    "Office_Products['unixReviewTime'] = unixReviewTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - meta_Home_and_Kitchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jlines = []\n",
    "# Abrir el archivo JSON en modo lectura\n",
    "with open('meta_Home_and_Kitchen.json', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Guardar las líneas del archivo JSON\n",
    "for line in lines:\n",
    "    jlines.append(line)\n",
    "\n",
    "# Extraer la información de las tres columnas principales del archivo json original\n",
    "category = []\n",
    "tech1 = []\n",
    "description = []\n",
    "fit = []\n",
    "title = []\n",
    "also_buy = []\n",
    "tech2 = []\n",
    "brand = []\n",
    "feature = []\n",
    "rank = []\n",
    "also_view = []\n",
    "main_cat = []\n",
    "similar_item = []\n",
    "date = []\n",
    "price = []\n",
    "asin = []\n",
    "imageURL = []\n",
    "imageURLHighRes = []\n",
    "\n",
    "for line in jlines:\n",
    "    data = json.loads(line)  # Analizar la línea como JSON\n",
    "    # Extraer los datos del JSON\n",
    "    category.append(data[\"category\"])\n",
    "    tech1.append(data[\"tech1\"])\n",
    "    description.append(data[\"description\"][0] if data[\"description\"] else \"\")  # Manejar la lista vacía\n",
    "    fit.append(data[\"fit\"])\n",
    "    title.append(data[\"title\"])\n",
    "    also_buy.append(data[\"also_buy\"])\n",
    "    tech2.append(data[\"tech2\"])\n",
    "    brand.append(data[\"brand\"])\n",
    "    feature.append(data[\"feature\"])\n",
    "    rank.append(data[\"rank\"])\n",
    "    also_view.append(data[\"also_view\"])\n",
    "    main_cat.append(data[\"main_cat\"])\n",
    "    similar_item.append(data[\"similar_item\"])\n",
    "    date.append(data[\"date\"])\n",
    "    price.append(data[\"price\"])\n",
    "    asin.append(data[\"asin\"])\n",
    "    imageURL.append(data[\"imageURL\"])\n",
    "    imageURLHighRes.append(data[\"imageURLHighRes\"])\n",
    "\n",
    "# Crear el DataFrame\n",
    "meta_Home_and_Kitchen = pd.DataFrame()\n",
    "meta_Home_and_Kitchen['category'] = category\n",
    "meta_Home_and_Kitchen['tech1'] = tech1\n",
    "meta_Home_and_Kitchen['description'] = description\n",
    "meta_Home_and_Kitchen['fit'] = fit\n",
    "meta_Home_and_Kitchen['title'] = title\n",
    "meta_Home_and_Kitchen['also_buy'] = also_buy\n",
    "meta_Home_and_Kitchen['tech2'] = tech2\n",
    "meta_Home_and_Kitchen['brand'] = brand\n",
    "meta_Home_and_Kitchen['feature'] = feature\n",
    "meta_Home_and_Kitchen['rank'] = rank\n",
    "meta_Home_and_Kitchen['also_view'] = also_view\n",
    "meta_Home_and_Kitchen['main_cat'] = main_cat\n",
    "meta_Home_and_Kitchen['similar_item'] = similar_item\n",
    "meta_Home_and_Kitchen['date'] = date\n",
    "meta_Home_and_Kitchen['price'] = price\n",
    "meta_Home_and_Kitchen['asin'] = asin\n",
    "meta_Home_and_Kitchen['imageURL'] = imageURL\n",
    "meta_Home_and_Kitchen['imageURLHighRes'] = imageURLHighRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - meta_Kindle_Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jlines = []\n",
    "# Abrir el archivo JSON en modo lectura\n",
    "with open('meta_Kindle_Store.json', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Guardar las líneas del archivo JSON\n",
    "for line in lines:\n",
    "    jlines.append(line)\n",
    "\n",
    "# Extraer la información de las tres columnas principales del archivo json original\n",
    "category = []\n",
    "tech1 = []\n",
    "description = []\n",
    "fit = []\n",
    "title = []\n",
    "also_buy = []\n",
    "tech2 = []\n",
    "brand = []\n",
    "feature = []\n",
    "rank = []\n",
    "also_view = []\n",
    "main_cat = []\n",
    "similar_item = []\n",
    "date = []\n",
    "price = []\n",
    "asin = []\n",
    "imageURL = []\n",
    "imageURLHighRes = []\n",
    "\n",
    "for line in jlines:\n",
    "    data = json.loads(line)  # Analizar la línea como JSON\n",
    "    # Extraer los datos del JSON\n",
    "    category.append(data[\"category\"])\n",
    "    tech1.append(data[\"tech1\"])\n",
    "    description.append(data[\"description\"][0] if data[\"description\"] else \"\")  # Manejar la lista vacía\n",
    "    fit.append(data[\"fit\"])\n",
    "    title.append(data[\"title\"])\n",
    "    also_buy.append(data[\"also_buy\"])\n",
    "    tech2.append(data[\"tech2\"])\n",
    "    brand.append(data[\"brand\"])\n",
    "    feature.append(data[\"feature\"])\n",
    "    rank.append(data[\"rank\"])\n",
    "    also_view.append(data[\"also_view\"])\n",
    "    main_cat.append(data[\"main_cat\"])\n",
    "    similar_item.append(data[\"similar_item\"])\n",
    "    date.append(data[\"date\"])\n",
    "    price.append(data[\"price\"])\n",
    "    asin.append(data[\"asin\"])\n",
    "    imageURL.append(data[\"imageURL\"])\n",
    "    imageURLHighRes.append(data[\"imageURLHighRes\"])\n",
    "\n",
    "# Crear el DataFrame\n",
    "meta_Kindle_Store = pd.DataFrame()\n",
    "meta_Kindle_Store['category'] = category\n",
    "meta_Kindle_Store['tech1'] = tech1\n",
    "meta_Kindle_Store['description'] = description\n",
    "meta_Kindle_Store['fit'] = fit\n",
    "meta_Kindle_Store['title'] = title\n",
    "meta_Kindle_Store['also_buy'] = also_buy\n",
    "meta_Kindle_Store['tech2'] = tech2\n",
    "meta_Kindle_Store['brand'] = brand\n",
    "meta_Kindle_Store['feature'] = feature\n",
    "meta_Kindle_Store['rank'] = rank\n",
    "meta_Kindle_Store['also_view'] = also_view\n",
    "meta_Kindle_Store['main_cat'] = main_cat\n",
    "meta_Kindle_Store['similar_item'] = similar_item\n",
    "meta_Kindle_Store['date'] = date\n",
    "meta_Kindle_Store['price'] = price\n",
    "meta_Kindle_Store['asin'] = asin\n",
    "meta_Kindle_Store['imageURL'] = imageURL\n",
    "meta_Kindle_Store['imageURLHighRes'] = imageURLHighRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - meta_Office_Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jlines = []\n",
    "# Abrir el archivo JSON en modo lectura\n",
    "with open('meta_Office_Products.json', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Guardar las líneas del archivo JSON\n",
    "for line in lines:\n",
    "    jlines.append(line)\n",
    "\n",
    "# Extraer la información de las tres columnas principales del archivo json original\n",
    "category = []\n",
    "tech1 = []\n",
    "description = []\n",
    "fit = []\n",
    "title = []\n",
    "also_buy = []\n",
    "tech2 = []\n",
    "brand = []\n",
    "feature = []\n",
    "rank = []\n",
    "also_view = []\n",
    "main_cat = []\n",
    "similar_item = []\n",
    "date = []\n",
    "price = []\n",
    "asin = []\n",
    "imageURL = []\n",
    "imageURLHighRes = []\n",
    "\n",
    "for line in jlines:\n",
    "    data = json.loads(line)  # Analizar la línea como JSON\n",
    "    # Extraer los datos del JSON\n",
    "    category.append(data[\"category\"])\n",
    "    tech1.append(data[\"tech1\"])\n",
    "    description.append(data[\"description\"][0] if data[\"description\"] else \"\")  # Manejar la lista vacía\n",
    "    fit.append(data[\"fit\"])\n",
    "    title.append(data[\"title\"])\n",
    "    also_buy.append(data[\"also_buy\"])\n",
    "    tech2.append(data[\"tech2\"])\n",
    "    brand.append(data[\"brand\"])\n",
    "    feature.append(data[\"feature\"])\n",
    "    rank.append(data[\"rank\"])\n",
    "    also_view.append(data[\"also_view\"])\n",
    "    main_cat.append(data[\"main_cat\"])\n",
    "    similar_item.append(data[\"similar_item\"])\n",
    "    date.append(data[\"date\"])\n",
    "    price.append(data[\"price\"])\n",
    "    asin.append(data[\"asin\"])\n",
    "    imageURL.append(data[\"imageURL\"])\n",
    "    imageURLHighRes.append(data[\"imageURLHighRes\"])\n",
    "\n",
    "# Crear el DataFrame\n",
    "meta_Office_Products = pd.DataFrame()\n",
    "meta_Office_Products['category'] = category\n",
    "meta_Office_Products['tech1'] = tech1\n",
    "meta_Office_Products['description'] = description\n",
    "meta_Office_Products['fit'] = fit\n",
    "meta_Office_Products['title'] = title\n",
    "meta_Office_Products['also_buy'] = also_buy\n",
    "meta_Office_Products['tech2'] = tech2\n",
    "meta_Office_Products['brand'] = brand\n",
    "meta_Office_Products['feature'] = feature\n",
    "meta_Office_Products['rank'] = rank\n",
    "meta_Office_Products['also_view'] = also_view\n",
    "meta_Office_Products['main_cat'] = main_cat\n",
    "meta_Office_Products['similar_item'] = similar_item\n",
    "meta_Office_Products['date'] = date\n",
    "meta_Office_Products['price'] = price\n",
    "meta_Office_Products['asin'] = asin\n",
    "meta_Office_Products['imageURL'] = imageURL\n",
    "meta_Office_Products['imageURLHighRes'] = imageURLHighRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -   meta_Industrial_and_Scientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jlines = []\n",
    "# Abrir el archivo JSON en modo lectura\n",
    "with open('meta_Industrial_and_Scientific.json', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Guardar las líneas del archivo JSON\n",
    "for line in lines:\n",
    "    jlines.append(line)\n",
    "\n",
    "# Extraer la información de las tres columnas principales del archivo json original\n",
    "category = []\n",
    "tech1 = []\n",
    "description = []\n",
    "fit = []\n",
    "title = []\n",
    "also_buy = []\n",
    "tech2 = []\n",
    "brand = []\n",
    "feature = []\n",
    "rank = []\n",
    "also_view = []\n",
    "main_cat = []\n",
    "similar_item = []\n",
    "date = []\n",
    "price = []\n",
    "asin = []\n",
    "imageURL = []\n",
    "imageURLHighRes = []\n",
    "\n",
    "for line in jlines:\n",
    "    data = json.loads(line)  # Analizar la línea como JSON\n",
    "    # Extraer los datos del JSON\n",
    "    category.append(data[\"category\"])\n",
    "    tech1.append(data[\"tech1\"])\n",
    "    description.append(data[\"description\"][0] if data[\"description\"] else \"\")  # Manejar la lista vacía\n",
    "    fit.append(data[\"fit\"])\n",
    "    title.append(data[\"title\"])\n",
    "    also_buy.append(data[\"also_buy\"])\n",
    "    tech2.append(data[\"tech2\"])\n",
    "    brand.append(data[\"brand\"])\n",
    "    feature.append(data[\"feature\"])\n",
    "    rank.append(data[\"rank\"])\n",
    "    also_view.append(data[\"also_view\"])\n",
    "    main_cat.append(data[\"main_cat\"])\n",
    "    similar_item.append(data[\"similar_item\"])\n",
    "    date.append(data[\"date\"])\n",
    "    price.append(data[\"price\"])\n",
    "    asin.append(data[\"asin\"])\n",
    "    imageURL.append(data[\"imageURL\"])\n",
    "    imageURLHighRes.append(data[\"imageURLHighRes\"])\n",
    "\n",
    "# Crear el DataFrame\n",
    "meta_Industrial_and_Scientific = pd.DataFrame()\n",
    "meta_Industrial_and_Scientific['category'] = category\n",
    "meta_Industrial_and_Scientific['tech1'] = tech1\n",
    "meta_Industrial_and_Scientific['description'] = description\n",
    "meta_Industrial_and_Scientific['fit'] = fit\n",
    "meta_Industrial_and_Scientific['title'] = title\n",
    "meta_Industrial_and_Scientific['also_buy'] = also_buy\n",
    "meta_Industrial_and_Scientific['tech2'] = tech2\n",
    "meta_Industrial_and_Scientific['brand'] = brand\n",
    "meta_Industrial_and_Scientific['feature'] = feature\n",
    "meta_Industrial_and_Scientific['rank'] = rank\n",
    "meta_Industrial_and_Scientific['also_view'] = also_view\n",
    "meta_Industrial_and_Scientific['main_cat'] = main_cat\n",
    "meta_Industrial_and_Scientific['similar_item'] = similar_item\n",
    "meta_Industrial_and_Scientific['date'] = date\n",
    "meta_Industrial_and_Scientific['price'] = price\n",
    "meta_Industrial_and_Scientific['asin'] = asin\n",
    "meta_Industrial_and_Scientific['imageURL'] = imageURL\n",
    "meta_Industrial_and_Scientific['imageURLHighRes'] = imageURLHighRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - meta_Appliances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jlines = []\n",
    "# Abrir el archivo JSON en modo lectura\n",
    "with open('meta_Appliances.json', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Guardar las líneas del archivo JSON\n",
    "for line in lines:\n",
    "    jlines.append(line)\n",
    "\n",
    "# Extraer la información de las tres columnas principales del archivo json original\n",
    "category = []\n",
    "tech1 = []\n",
    "description = []\n",
    "fit = []\n",
    "title = []\n",
    "also_buy = []\n",
    "tech2 = []\n",
    "brand = []\n",
    "feature = []\n",
    "rank = []\n",
    "also_view = []\n",
    "main_cat = []\n",
    "similar_item = []\n",
    "date = []\n",
    "price = []\n",
    "asin = []\n",
    "imageURL = []\n",
    "imageURLHighRes = []\n",
    "\n",
    "for line in jlines:\n",
    "    data = json.loads(line)  # Analizar la línea como JSON\n",
    "    # Extraer los datos del JSON\n",
    "    category.append(data[\"category\"])\n",
    "    tech1.append(data[\"tech1\"])\n",
    "    description.append(data[\"description\"][0] if data[\"description\"] else \"\")  # Manejar la lista vacía\n",
    "    fit.append(data[\"fit\"])\n",
    "    title.append(data[\"title\"])\n",
    "    also_buy.append(data[\"also_buy\"])\n",
    "    tech2.append(data[\"tech2\"])\n",
    "    brand.append(data[\"brand\"])\n",
    "    feature.append(data[\"feature\"])\n",
    "    rank.append(data[\"rank\"])\n",
    "    also_view.append(data[\"also_view\"])\n",
    "    main_cat.append(data[\"main_cat\"])\n",
    "    similar_item.append(data[\"similar_item\"])\n",
    "    date.append(data[\"date\"])\n",
    "    price.append(data[\"price\"])\n",
    "    asin.append(data[\"asin\"])\n",
    "    imageURL.append(data[\"imageURL\"])\n",
    "    imageURLHighRes.append(data[\"imageURLHighRes\"])\n",
    "\n",
    "# Crear el DataFrame\n",
    "meta_Appliances = pd.DataFrame()\n",
    "meta_Appliances['category'] = category\n",
    "meta_Appliances['tech1'] = tech1\n",
    "meta_Appliances['description'] = description\n",
    "meta_Appliances['fit'] = fit\n",
    "meta_Appliances['title'] = title\n",
    "meta_Appliances['also_buy'] = also_buy\n",
    "meta_Appliances['tech2'] = tech2\n",
    "meta_Appliances['brand'] = brand\n",
    "meta_Appliances['feature'] = feature\n",
    "meta_Appliances['rank'] = rank\n",
    "meta_Appliances['also_view'] = also_view\n",
    "meta_Appliances['main_cat'] = main_cat\n",
    "meta_Appliances['similar_item'] = similar_item\n",
    "meta_Appliances['date'] = date\n",
    "meta_Appliances['price'] = price\n",
    "meta_Appliances['asin'] = asin\n",
    "meta_Appliances['imageURL'] = imageURL\n",
    "meta_Appliances['imageURLHighRes'] = imageURLHighRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Limpieza de datos (Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, evaluaremos la utilidad de los datos presentes en los DataFrames y decidiremos si es necesario eliminar ciertos elementos para optimizar el uso de memoria. También realizaremos la estandarización de los datos, que incluye la conversión a minúsculas, para garantizar una consistencia en el formato.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En todos los DataFrames de las secciones de los elementos, se identificaron patrones comunes. Por ejemplo, se encontraron duplicados en las columnas reviewerID y comentario, los cuales deben eliminarse. Además, se observó que las columnas reviewTime y unixReviewTime hacen referencia a la misma fecha, prefiriendo la columna unixReviewTime debido a su representación numérica y menor consumo de espacio. La columna style proporciona información sobre el formato del artículo, pero no aporta datos relevantes para nuestra base de datos. Del mismo modo, la columna verified indica si el comentario está verificado, información que no es necesaria en nuestro conjunto de datos. Por último, el campo reviewerName (nombre de usuario) tampoco resulta relevante para nuestros fines.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Office_Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados\n",
    "Office_Products.drop_duplicates(subset=['reviewerID', 'reviewText'], keep='first', inplace=True)\n",
    "# Reiniciar los indices del DataFrame\n",
    "Office_Products.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "del Office_Products['reviewTime']\n",
    "del Office_Products['style']\n",
    "del Office_Products['verified']\n",
    "del Office_Products['reviewerName']\n",
    "\n",
    "# Hacer que todo el DataFrame esté en minúscula\n",
    "Office_Products = Office_Products.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir el DataFrame a parquet para ahorrar aún más espacio\n",
    "Office_Products.to_parquet('Patio_Lawn_and_Garden.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Home_and_Kitchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados\n",
    "Home_and_Kitchen.drop_duplicates(subset=['reviewerID', 'reviewText'], keep='first', inplace=True)\n",
    "# Reiniciar los indices del DataFrame\n",
    "Home_and_Kitchen.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "del Home_and_Kitchen['reviewTime']\n",
    "del Home_and_Kitchen['style']\n",
    "del Home_and_Kitchen['verified']\n",
    "del Home_and_Kitchen['reviewerName']\n",
    "\n",
    "# Hacer que todo el DataFrame esté en minúscula\n",
    "Home_and_Kitchen = Home_and_Kitchen.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir el DataFrame a parquet para ahorrar aún más espacio\n",
    "Home_and_Kitchen.to_parquet('Home_and_Kitchen.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Kindle_Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados\n",
    "Kindle_Store.drop_duplicates(subset=['reviewerID', 'reviewText'], keep='first', inplace=True)\n",
    "# Reiniciar los indices del DataFrame\n",
    "Kindle_Store.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "del Kindle_Store['reviewTime']\n",
    "del Kindle_Store['style']\n",
    "del Kindle_Store['verified']\n",
    "del Kindle_Store['reviewerName']\n",
    "\n",
    "# Hacer que todo el DataFrame esté en minúscula\n",
    "Kindle_Store = Kindle_Store.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir el DataFrame a parquet para ahorrar aún más espacio\n",
    "Kindle_Store.to_parquet('Kindle_Store.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Industrial_and_Scientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados\n",
    "Industrial_and_Scientific.drop_duplicates(subset=['reviewerID', 'reviewText'], keep='first', inplace=True)\n",
    "# Reiniciar los indices del DataFrame\n",
    "Industrial_and_Scientific.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "del Industrial_and_Scientific['reviewTime']\n",
    "del Industrial_and_Scientific['style']\n",
    "del Industrial_and_Scientific['verified']\n",
    "del Industrial_and_Scientific['reviewerName']\n",
    "\n",
    "# Hacer que todo el DataFrame esté en minúscula\n",
    "Industrial_and_Scientific = Industrial_and_Scientific.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir el DataFrame a parquet para ahorrar aún más espacio\n",
    "Industrial_and_Scientific.to_parquet('Industrial_and_Scientific.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Appliances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados\n",
    "Appliances.drop_duplicates(subset=['reviewerID', 'reviewText'], keep='first', inplace=True)\n",
    "# Reiniciar los indices del DataFrame\n",
    "Appliances.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "del Appliances['reviewTime']\n",
    "del Appliances['style']\n",
    "del Appliances['verified']\n",
    "del Appliances['reviewerName']\n",
    "\n",
    "# Hacer que todo el DataFrame esté en minúscula\n",
    "Appliances = Appliances.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir el DataFrame a parquet para ahorrar aún más espacio\n",
    "Appliances.to_parquet('Appliances.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, procederemos a abordar los archivos \"meta\". En esta etapa, se llevará a cabo la eliminación de las siguientes columnas en dichos archivos: category, tech1, fit, also_buy, tech2, feature, rank, also_view, similar_item, date, imageURL, e imageURLHighRes. Esta decisión se basa en la falta de información relevante que estas columnas proporcionan o en el alto porcentaje de valores nulos que contienen, lo que las hace prescindibles para nuestros propósitos.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asimismo, se procede a la transformación de los datos en la columna \"price\". Esta columna contiene valores en formato de moneda, como \"$44.95\", los cuales se convierten a tipo float con el fin de optimizar el uso de espacio. Para llevar a cabo esta conversión, se ha creado una función denominada \"convert_to_float\". Además de esta transformación, se estandarizan todos los datos a minúsculas y se realiza la eliminación de duplicados para garantizar la consistencia y calidad de la información.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_float(value):\n",
    "    try:\n",
    "        return float(value.replace('$', ''))\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - meta_Home_and_Kitchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas innecesarias\n",
    "del meta_Home_and_Kitchen['category']\n",
    "del meta_Home_and_Kitchen['tech1']\n",
    "del meta_Home_and_Kitchen['fit']\n",
    "del meta_Home_and_Kitchen['also_buy']\n",
    "del meta_Home_and_Kitchen['tech2']\n",
    "del meta_Home_and_Kitchen['feature']\n",
    "del meta_Home_and_Kitchen['rank']\n",
    "del meta_Home_and_Kitchen['also_view']\n",
    "del meta_Home_and_Kitchen['similar_item']\n",
    "del meta_Home_and_Kitchen['date']\n",
    "del meta_Home_and_Kitchen['imageURLHighRes']\n",
    "\n",
    "# Eliminar duplicados\n",
    "meta_Home_and_Kitchen.drop_duplicates(subset=['title', 'asin'], keep='first', inplace=True)\n",
    "# Reiniciar los indices del DataFrame\n",
    "meta_Home_and_Kitchen.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Realizar operaciones en la columna \"price\" antes de la conversión\n",
    "meta_Home_and_Kitchen['price'] = meta_Home_and_Kitchen['price'].apply(lambda x: str(x).replace('$', '') if isinstance(x, str) else str(x))\n",
    "# Convertir la columna \"price\" a float\n",
    "meta_Home_and_Kitchen['price'] = meta_Home_and_Kitchen['price'].apply(convert_to_float)\n",
    "\n",
    "# Hacer que todo el DataFrame esté en minúscula\n",
    "meta_Home_and_Kitchen = meta_Home_and_Kitchen.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir el DataFrame a parquet para ahorrar aún más espacio\n",
    "meta_Home_and_Kitchen.to_parquet('meta_Home_and_Kitchen.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - meta_Kindle_Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas innecesarias\n",
    "del meta_Kindle_Store['category']\n",
    "del meta_Kindle_Store['tech1']\n",
    "del meta_Kindle_Store['fit']\n",
    "del meta_Kindle_Store['also_buy']\n",
    "del meta_Kindle_Store['tech2']\n",
    "del meta_Kindle_Store['feature']\n",
    "del meta_Kindle_Store['rank']\n",
    "del meta_Kindle_Store['also_view']\n",
    "del meta_Kindle_Store['similar_item']\n",
    "del meta_Kindle_Store['date']\n",
    "del meta_Kindle_Store['imageURL']\n",
    "del meta_Kindle_Store['imageURLHighRes']\n",
    "\n",
    "# Eliminar duplicados\n",
    "meta_Kindle_Store.drop_duplicates(subset=['title', 'asin'], keep='first', inplace=True)\n",
    "# Reiniciar los indices del DataFrame\n",
    "meta_Kindle_Store.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Realizar operaciones en la columna \"price\" antes de la conversión\n",
    "meta_Kindle_Store['price'] = meta_Kindle_Store['price'].apply(lambda x: str(x).replace('$', '') if isinstance(x, str) else str(x))\n",
    "# Convertir la columna \"price\" a float\n",
    "meta_Kindle_Store['price'] = meta_Kindle_Store['price'].apply(convert_to_float)\n",
    "\n",
    "# Hacer que todo el DataFrame esté en minúscula\n",
    "meta_Kindle_Store = meta_Kindle_Store.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir el DataFrame a parquet para ahorrar aún más espacio\n",
    "meta_Kindle_Store.to_parquet('meta_Kindle_Store.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - meta_Office_Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas innecesarias\n",
    "del meta_Office_Products['category']\n",
    "del meta_Office_Products['tech1']\n",
    "del meta_Office_Products['fit']\n",
    "del meta_Office_Products['also_buy']\n",
    "del meta_Office_Products['tech2']\n",
    "del meta_Office_Products['feature']\n",
    "del meta_Office_Products['rank']\n",
    "del meta_Office_Products['also_view']\n",
    "del meta_Office_Products['similar_item']\n",
    "del meta_Office_Products['date']\n",
    "del meta_Office_Products['imageURL']\n",
    "del meta_Office_Products['imageURLHighRes']\n",
    "\n",
    "# Eliminar duplicados\n",
    "meta_Office_Products.drop_duplicates(subset=['title', 'asin'], keep='first', inplace=True)\n",
    "# Reiniciar los indices del DataFrame\n",
    "meta_Office_Products.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Realizar operaciones en la columna \"price\" antes de la conversión\n",
    "meta_Office_Products['price'] = meta_Office_Products['price'].apply(lambda x: str(x).replace('$', '') if isinstance(x, str) else str(x))\n",
    "# Convertir la columna \"price\" a float\n",
    "meta_Office_Products['price'] = meta_Office_Products['price'].apply(convert_to_float)\n",
    "\n",
    "# Hacer que todo el DataFrame esté en minúscula\n",
    "meta_Office_Products = meta_Office_Products.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir el DataFrame a parquet para ahorrar aún más espacio\n",
    "meta_Office_Products.to_parquet('meta_Office_Products.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - meta_Industrial_and_Scientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas innecesarias\n",
    "del meta_Industrial_and_Scientific['category']\n",
    "del meta_Industrial_and_Scientific['tech1']\n",
    "del meta_Industrial_and_Scientific['fit']\n",
    "del meta_Industrial_and_Scientific['also_buy']\n",
    "del meta_Industrial_and_Scientific['tech2']\n",
    "del meta_Industrial_and_Scientific['feature']\n",
    "del meta_Industrial_and_Scientific['rank']\n",
    "del meta_Industrial_and_Scientific['also_view']\n",
    "del meta_Industrial_and_Scientific['similar_item']\n",
    "del meta_Industrial_and_Scientific['date']\n",
    "del meta_Industrial_and_Scientific['imageURL']\n",
    "del meta_Industrial_and_Scientific['imageURLHighRes']\n",
    "\n",
    "# Eliminar duplicados\n",
    "meta_Industrial_and_Scientific.drop_duplicates(subset=['title', 'asin'], keep='first', inplace=True)\n",
    "# Reiniciar los indices del DataFrame\n",
    "meta_Industrial_and_Scientific.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Realizar operaciones en la columna \"price\" antes de la conversión\n",
    "meta_Industrial_and_Scientific['price'] = meta_Industrial_and_Scientific['price'].apply(lambda x: str(x).replace('$', '') if isinstance(x, str) else str(x))\n",
    "# Convertir la columna \"price\" a float\n",
    "meta_Industrial_and_Scientific['price'] = meta_Industrial_and_Scientific['price'].apply(convert_to_float)\n",
    "\n",
    "# Hacer que todo el DataFrame esté en minúscula\n",
    "meta_Industrial_and_Scientific = meta_Industrial_and_Scientific.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir el DataFrame a parquet para ahorrar aún más espacio\n",
    "meta_Industrial_and_Scientific.to_parquet('meta_Industrial_and_Scientific.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - meta_Appliances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas innecesarias\n",
    "del meta_Appliances['category']\n",
    "del meta_Appliances['tech1']\n",
    "del meta_Appliances['fit']\n",
    "del meta_Appliances['also_buy']\n",
    "del meta_Appliances['tech2']\n",
    "del meta_Appliances['feature']\n",
    "del meta_Appliances['rank']\n",
    "del meta_Appliances['also_view']\n",
    "del meta_Appliances['similar_item']\n",
    "del meta_Appliances['date']\n",
    "del meta_Appliances['imageURL']\n",
    "del meta_Appliances['imageURLHighRes']\n",
    "\n",
    "# Eliminar duplicados\n",
    "meta_Appliances.drop_duplicates(subset=['title', 'asin'], keep='first', inplace=True)\n",
    "# Reiniciar los indices del DataFrame\n",
    "meta_Appliances.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Realizar operaciones en la columna \"price\" antes de la conversión\n",
    "meta_Appliances['price'] = meta_Appliances['price'].apply(lambda x: str(x).replace('$', '') if isinstance(x, str) else str(x))\n",
    "# Convertir la columna \"price\" a float\n",
    "meta_Appliances['price'] = meta_Appliances['price'].apply(convert_to_float)\n",
    "\n",
    "# Hacer que todo el DataFrame esté en minúscula\n",
    "meta_Appliances = meta_Appliances.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir el DataFrame a parquet para ahorrar aún más espacio\n",
    "meta_Appliances.to_parquet('meta_Appliances.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Carga de los datos (Load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos datos ya procesados han sido cargados a google cloud para posterior manejo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
